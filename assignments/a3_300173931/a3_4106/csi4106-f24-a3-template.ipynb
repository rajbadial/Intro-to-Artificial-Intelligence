{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CSI 4106 Introduction to Artificial Intelligence** <br/>\n",
    "*Assignment 3: Neural Networks*\n",
    "\n",
    "# Identification\n",
    "\n",
    "Name: Raj Badial<br/>\n",
    "Student Number: 300173931\n",
    "Tasks: Steps 1-7\n",
    "\n",
    "Name: Shereen Etemad<br/>\n",
    "Student Number: 300186291\n",
    "Tasks: Steps 8 & 9\n",
    "\n",
    "Split this way because 8 and 9 took the longest by far. We both helped each other throughout the assignment however.\n",
    "\n",
    "## 1. Exploratory Analysis\n",
    "\n",
    "### Loading the dataset\n",
    "\n",
    "A custom dataset has been created for this assignment. It has been made available on a public GitHub repository:\n",
    "\n",
    "- [github.com/turcotte/csi4106-f24/tree/main/assignments-data/a3](https://github.com/turcotte/csi4106-f24/tree/main/assignments-data/a3)\n",
    "\n",
    "Access and read the dataset directly from this GitHub repository in your Jupyter notebook.\n",
    "\n",
    "You can use this code cell for you import statements and other initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout # type: ignore\n",
    "from tensorflow.keras.callbacks import EarlyStopping # type: ignore\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "import tensorflow as tf # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.regularizers import l2 # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "\n",
    "#1: Load the Dataset\n",
    "\n",
    "# URLs for the datasets (replace with actual URLs if these are not correct)\n",
    "train_url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a3/cb513_train.csv'\n",
    "validation_url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a3/cb513_valid.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/turcotte/csi4106-f24/refs/heads/main/assignments-data/a3/cb513_test.csv'\n",
    "\n",
    "# Load the datasets\n",
    "train_data = pd.read_csv(train_url, header=None)\n",
    "validation_data = pd.read_csv(validation_url, header=None)\n",
    "test_data = pd.read_csv(test_url, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n",
    "\n",
    "2. **Shuffling the Rows**:\n",
    "\n",
    "    - Since examples are generated by sliding a window across each protein sequence, most adjacent examples originate from the same protein and share 20 positions. To mitigate the potential negative impact on model training, the initial step involves shuffling the **rows** of the data matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "shuffled_training = train_data.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Scaling of Numerical Features**:\n",
    "\n",
    "    - Since all 462 features are proportions represented as values between 0 and 1, scaling may not be necessary. In our evaluations, using [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) actually degraded model performance. Within your pipeline, compare the effects of not scaling the data versus applying [MinMaxScaler](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MinMaxScaler.html). In the interest of time, a single experiment will suffice. It is important to note that when scaling is applied, a uniform method should be used across all columns, given their homogeneous nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "y = shuffled_training.iloc[:, 0]    # target vector\n",
    "X = shuffled_training.iloc[:, 1:]   # features\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Option 1: Without scaling\n",
    "model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "no_scaling_score = model.score(X_val, y_val)\n",
    "\n",
    "# Option 2: With MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "scaling_score = model.score(X_val_scaled, y_val)\n",
    "\n",
    "# Compare results\n",
    "print(\"Score without scaling:\", no_scaling_score)\n",
    "print(\"Score with MinMaxScaler:\", scaling_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score without scaling: 0.5514195042456471 <br>\n",
    "Score with MinMaxScaler: 0.5514195042456471"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Isolating the Target and the Data**:\n",
    "\n",
    "    - In the CSV files, the target and data are combined. To prepare for our machine learning experiments, separate the training data $X$ and the target vector $y$ for each of the three datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 4: Isolating the data for each dataset (already done for training in step 3, labeled just as y and x)\n",
    "y_validation = validation_data.iloc[:, 0]    # target vector\n",
    "X_validation = validation_data.iloc[:, 1:]   # features\n",
    "\n",
    "y_test = test_data.iloc[:, 0]    # target vector\n",
    "X_test = test_data.iloc[:, 1:]   # features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development & Evaluation\n",
    "\n",
    "5. **Model Development**:\n",
    "\n",
    "    - **Dummy Model**: Implement a model utilizing the [DummyClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html). This model disregards the input data and predicts the majority class. Such model is sometimes called a straw man model.\n",
    "\n",
    "    - **Basline Model**: As a baseline model, select one of the previously studied machine learning algorithms: Decision Trees, K-Nearest Neighbors (KNN), or Logistic Regression. Use the default parameters provided by scikit-learn to train each model as a baseline. Why did you choose this particular classifier? Why do you think it should be appropriate for this specific task?\n",
    "\n",
    "    - **Neural Network Model**: Utilizing [Keras](https://keras.io) and [TensorFlow](https://www.tensorflow.org), construct a sequential model comprising an input layer, a hidden layer, and an output layer. The input layer should consist of 462 nodes, reflecting the 462 attributes of each example. The hidden layer should include 8 nodes and employ the default activation function. The output layer should contain three nodes, corresponding to the three classes: helix (0), sheet (1), and coil (2). Apply the softmax activation function to the output layer to ensure that the outputs are treated as probabilities, with their sum equaling 1 for each training example.\n",
    "\n",
    "    We therefore have three models: dummy, baseline, and neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 5\n",
    "# Implementing Dummy model\n",
    "dummy_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_model.fit(X_train, y_train)\n",
    "y_val_pred_dummy = dummy_model.predict(X_val)\n",
    "\n",
    "# Implementing Baseline model (with DecisionTree)\n",
    "baseline_model = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "baseline_model.fit(X_train, y_train)\n",
    "y_val_pred_baseline = baseline_model.predict(X_val)\n",
    "\n",
    "# Using decision tree as many of the relationships are likely non-linear which decision tree is good at predicting, \n",
    "# while Logistic Regression is better for linear predictions. And KNN is better for smaller datasets where the data naturally \n",
    "# clusters around each class. Because of high-dimensionality KNN may not be the best fit. Decision Tree is prone to overfitting however.\n",
    "\n",
    "# Implementing Neural Network Model\n",
    "def create_neural_network(hidden_nodes):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(462,)))                                # Input layer with 462 nodes\n",
    "    model.add(Dense(hidden_nodes, activation='relu'))             # Hidden layer with 8 nodes (using hidden_nodes to reuse this function later for step 8)\n",
    "    model.add(Dense(3, activation='softmax'))                     # Output layer with 3 nodes\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "nn_model = create_neural_network(8)\n",
    "history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val)) # train the model\n",
    "y_val_pred_nn = nn_model.predict(X_val).argmax(axis=1) # because we're using softmax in the NNM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Model Evaluation**:\n",
    "\n",
    "    - Employ cross-validation to assess the performance of the baseline model. Select a small number of folds to prevent excessive computational demands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 6: Cross-Eval of Models\n",
    "# Dummy Model\n",
    "accuracy_dummy = accuracy_score(y_val, y_val_pred_dummy)\n",
    "precision_dummy = precision_score(y_val, y_val_pred_dummy, average='weighted', zero_division=1)\n",
    "recall_dummy = recall_score(y_val, y_val_pred_dummy, average='weighted')\n",
    "f1_dummy = f1_score(y_val, y_val_pred_dummy, average='weighted')\n",
    "\n",
    "print(\"\\nDummy Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_dummy:.2f}\")\n",
    "print(f\"Precision: {precision_dummy:.2f}\")\n",
    "print(f\"Recall: {recall_dummy:.2f}\")\n",
    "print(f\"F1 Score: {f1_dummy:.2f}\")\n",
    "\n",
    "# Baseline Model w 5 folds\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score), \n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_validate(baseline_model, X_train, y_train, cv=5, scoring=scoring)\n",
    "\n",
    "# Display average cross-validation scores\n",
    "print(\"\\nBaseline Model Performance:\")\n",
    "print(f\"Accuracy: {cv_results['test_accuracy'].mean():.4f}\")\n",
    "print(f\"Decision Tree CV Average Precision: {cv_results['test_precision'].mean():.4f}\")\n",
    "print(f\"Decision Tree CV Average Recall: {cv_results['test_recall'].mean():.4f}\")\n",
    "print(f\"Decision Tree CV Average F1-Score: {cv_results['test_f1'].mean():.4f}\")\n",
    "\n",
    "# NN Model\n",
    "accuracy_nn = accuracy_score(y_val, y_val_pred_nn)\n",
    "precision_nn = precision_score(y_val, y_val_pred_nn, average='weighted')\n",
    "recall_nn = recall_score(y_val, y_val_pred_nn, average='weighted')\n",
    "f1_nn = f1_score(y_val, y_val_pred_nn, average='weighted')\n",
    "\n",
    "print(\"\\nNeural Network Model Performance:\")\n",
    "print(f\"Accuracy: {accuracy_nn:.2f}\")\n",
    "print(f\"Precision: {precision_nn:.2f}\")\n",
    "print(f\"Recall: {recall_nn:.2f}\")\n",
    "print(f\"F1 Score: {f1_nn:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy Model Performance: <br>\n",
    "Accuracy: 0.42<br>\n",
    "Precision: 0.76<br>\n",
    "Recall: 0.42<br>\n",
    "F1 Score: 0.24<br>\n",
    "\n",
    "Baseline Model Performance:<br>\n",
    "Accuracy: 0.5552<br>\n",
    "Decision Tree CV Average Precision: 0.5517<br>\n",
    "Decision Tree CV Average Recall: 0.5552<br>\n",
    "Decision Tree CV Average F1-Score: 0.5466<br>\n",
    "\n",
    "Neural Network Model Performance:<br>\n",
    "Accuracy: 0.70<br>\n",
    "Precision: 0.70<br>\n",
    "Recall: 0.70<br>\n",
    "F1 Score: 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Assess the models using metrics such as precision, recall, and F1-score.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "\n",
    "7. **Baseline Model:**\n",
    "\n",
    "    - To ensure a fair comparison for our baseline model, we will examine how varying hyperparameter values affect its performance. This prevents the erroneous conclusion that neural networks inherently perform better, when in fact, appropriate hyperparameter tuning could enhance the baseline model's performance.\n",
    "\n",
    "    - Focus on the following relevant hyperparameters for each model:\n",
    "\n",
    "        - [DecisionTreeClassifier](https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html): `criterion` and `max_depth`.\n",
    "  \n",
    "        - [LogisticRegression](https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html): `penalty`, `max_iter`, and `tol`.\n",
    "  \n",
    "        - [KNeighborsClassifier](https://scikit-learn.org/dev/modules/generated/sklearn.neighbors.KNeighborsClassifier.html): `n_neighbors` and `weights`.\n",
    "\n",
    "    - Employ a grid search strategy or utilize scikit-learn's built-in methods [GridSearchCV](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html) to thoroughly evaluate all combinations of hyperparameter values. Cross-validation should be used to assess each combination.\n",
    "\n",
    "    - Quantify the performance of each hyperparameter configuration using precision, recall, and F1-score as metrics.\n",
    "\n",
    "    - Analyze the findings and offer insights into which hyperparameter configurations achieved optimal performance for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 7: Hyperparameter Optimization for Baseline model\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],   # Limits depth to avoid overfitting\n",
    "    'min_samples_split': [2, 5, 10]    # Controls when a node will be split\n",
    "}\n",
    "\n",
    "scoring = {\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with the Decision Tree model and 5-fold cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring=scoring,\n",
    "    refit='f1',                      # Refits on the best parameter setting based on F1 score\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1                        # Use all available CPU cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# best parameters found\n",
    "print(\"Best parameters found:\", grid_search.best_params_)\n",
    "\n",
    "# average precision and recall scores for the best model\n",
    "best_cv_results = grid_search.cv_results_\n",
    "best_index = grid_search.best_index_\n",
    "print(\"Best Cross-Validated Precision:\", best_cv_results['mean_test_precision'][best_index])\n",
    "print(\"Best Cross-Validated Recall:\", best_cv_results['mean_test_recall'][best_index])\n",
    "print(\"Best Cross-Validated F1 Score:\", best_cv_results['mean_test_f1'][best_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters found: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_split': 5}<br>\n",
    "Best Cross-Validated Precision: 0.554962721731411<br>\n",
    "Best Cross-Validated Recall: 0.5570424802596701<br>\n",
    "Best Cross-Validated F1 Score: 0.5471584699757781"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Neural Network:**\n",
    "\n",
    "    In our exploration and tuning of neural networks, we focus on the following hyperparameters:\n",
    "\n",
    "    - **Single hidden layer, varying the number of nodes**. \n",
    "\n",
    "        - Start with a single node in the hidden layer. Use a graph to depict the progression of loss and accuracy for both the training and validation sets, with the horizontal axis representing the number of training epochs and the vertical axis showing loss and accuracy. Training this network should be relatively fast, so let's conduct training for 50 epochs. Observing the graph, what do you conclude? Is the network underfitting or overfitting? Why?\n",
    "\n",
    "        - Repeat the above process using 2 and 4 nodes in the hidden layer. Use the same type of graph to document your observations regarding loss and accuracy.\n",
    "\n",
    "        - Start with 8 nodes in the hidden layer and progressively double the number of nodes until it surpasses the number of nodes in the input layer. This results in seven experiments and corresponding graphs for the following configurations: 8, 16, 32, 64, 128, 256, and 512 nodes. Document your observations throughout the process.\n",
    "        \n",
    "        - Ensure that the **number of training epochs** is adequate for **observing an increase in validation loss**. **Tip**: During model development, start with a small number of epochs, such as 5 or 10. Once the model appears to perform well, test with larger values, like 40 or 80 epochs, which proved reasonable in our tests. Based on your observations, consider conducting further experiments, if needed. How many epochs were ultimately necessary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 8 (Neural Network Experimentation)\n",
    "# Varying nodes in hidden layer:\n",
    "\n",
    "# List of node counts to try in the hidden layer\n",
    "node_counts = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]\n",
    "\n",
    "# Dictionary to store the validation accuracy for each configuration\n",
    "results = {}\n",
    "\n",
    "# Loop through each node configuration, train the model, and record the validation accuracy\n",
    "for nodes in node_counts:\n",
    "    print(f\"\\nTraining model with {nodes} hidden nodes...\")\n",
    "    nn_model = create_neural_network(hidden_nodes=nodes) # Same function from Step 5\n",
    "    history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    # Plot training and validation loss and accuracy\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss for {nodes} Nodes in Hidden Layer')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy for {nodes} Nodes in Hidden Layer')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2024-11-09 224237.png>) ![alt text](<Screenshot 2024-11-09 224340.png>) ![alt text](<Screenshot 2024-11-09 224450.png>) ![alt text](<Screenshot 2024-11-09 224608.png>) ![alt text](<Screenshot 2024-11-09 224728.png>) ![alt text](<Screenshot 2024-11-09 224849.png>) ![alt text](<Screenshot 2024-11-09 225010.png>) ![alt text](<Screenshot 2024-11-09 225149.png>) ![alt text](<Screenshot 2024-11-09 225350.png>) ![alt text](<Screenshot 2024-11-09 225640.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - **Varying the number of layers**.\n",
    "\n",
    "        - Conduct similar experiments as described above, but this time vary the number of layers from 1 to 4. Document your findings.\n",
    "\n",
    "        - How many nodes should each layer contain? Test at least two scenarios. Traditionally, a common strategy involved decreasing the number of nodes from the input layer to the output layer, often by halving, to create a pyramid-like structure. However, recent experience suggests that maintaining a constant number of nodes across all layers can perform equally well. Describe your observations. It is acceptable if both strategies yield similar performance results.\n",
    "\n",
    "        - Select one your models that exemplifies overfitting. In our experiments, we easily constructed a model achieving nearly 100% accuracy on the training data, yet showing no similar improvement on the validation set. Present this neural network along with its accuracy and loss graphs. Explain the reasoning for concluding that the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Varying number of layers:\n",
    "def create_neural_network2(num_layers, nodes_per_layer=8):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(462,)))  # Input layer with 462 features\n",
    "    \n",
    "    # Add the specified number of hidden layers with the same number of nodes per layer\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation='relu'))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))  # Output layer with 3 nodes for 3 classes\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# List of layer counts to try\n",
    "layer_counts = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Loop through each configuration, train the model, and plot the results\n",
    "for layers in layer_counts:\n",
    "    print(f\"\\nTraining model with {layers} hidden layers\")\n",
    "    nn_model = create_neural_network2(num_layers=layers)\n",
    "    history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    # Plot training and validation loss and accuracy\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss for {layers} Hidden Layers')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy for {layers} Hidden Layers')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2024-11-09 225759.png>) ![alt text](<Screenshot 2024-11-09 225913.png>) ![alt text](<Screenshot 2024-11-09 230024.png>) ![alt text](<Screenshot 2024-11-09 230139.png>) ![alt text](<Screenshot 2024-11-09 230256.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - **Activation function**.\n",
    "\n",
    "        - Present results for one of the configurations mentioned above by varying the activation function. Test at least `relu` (the default) and `sigmoid`. The choice of the specific model, including the number of layers and nodes, is at your discretion. Document your observations accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Changing the activation function\n",
    "def create_neural_network3(activation='relu', num_layers=2, nodes_per_layer=8):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(462,)))  # Input layer with 462 features\n",
    "    \n",
    "    # Hidden layers with the specified activation function\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation=activation))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax'))  # Output layer with 3 nodes for 3 classes\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# List of activation functions\n",
    "activation_functions = ['relu', 'sigmoid']\n",
    "\n",
    "# Loop through each activation function, train the model, and plot the results\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\nTraining model with {activation} activation function\")\n",
    "    nn_model = create_neural_network3(activation=activation)\n",
    "    history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    # Plot training and validation loss and accuracy\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss with {activation} Activation Function')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy with {activation} Activation Function')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2024-11-09 230403.png>) ![alt text](<Screenshot 2024-11-09 230510.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - **Regularization** in neural networks is a technique used to prevent overfitting.\n",
    "\n",
    "        - One technique involves adding a penalty to the loss function to discourage excessively complex models. Apply an `l2` penalty to some or all layers. Exercise caution, as overly aggressive penalties have been problematic in our experiments. Begin with the default `l2` value of 0.01, then reduce it to 0.001 and 1e-4. Select a specific model from the above experiments and present a case where you successfully reduced overfitting. Include a pair of graphs comparing results with and without regularization. Explain your rationale to conclude that overfitting has been reduced. Do not expect to completely eliminate overfitting. Again, this is a challenging dataset to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        - Dropout layers are a regularization technique in neural networks where a random subset of neurons is temporarily removed during training. This helps prevent overfitting by promoting redundancy and improving the network's ability to generalize to new data. Select a specific model from the above experiments where you have muliple layers and experiment adding one or of few dropout layers into your network. Experiment with two different rates, say 0.25 and 0.5. Document your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Regularization techniques\n",
    "def create_neural_network4(l2_penalty=0.0, dropout_rate=0.0):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(462,)))\n",
    "    model.add(Dense(8, activation='relu', kernel_regularizer=l2(l2_penalty)))  # L2 regularization on the hidden layer\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))  # Apply dropout if rate > 0\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define different regularization configurations to try\n",
    "regularization_configs = [\n",
    "    {'l2_penalty': 0.01, 'dropout_rate': 0.0}, # Only L2 regularization\n",
    "    {'l2_penalty': 0.001, 'dropout_rate': 0.0}, # Smaller L2 regularization\n",
    "    {'l2_penalty': 0.01, 'dropout_rate': 0.25}, # Both L2 and Dropout of 0.25\n",
    "    {'l2_penalty': 0.001, 'dropout_rate': 0.25}, # Smaller L2 with 0.25 dropoyt\n",
    "    {'l2_penalty': 0.01, 'dropout_rate': 0.5}, # L2 with 0.5 dropout\n",
    "    {'l2_penalty': 0.001, 'dropout_rate': 0.5}, # Smaller L2 with 0.5 dropout \n",
    "    {'l2_penalty': 0.0, 'dropout_rate': 0.25}, # Only Dropout of 0.25\n",
    "    {'l2_penalty': 0.0, 'dropout_rate': 0.5} # Only Dropout of 0.5\n",
    "]\n",
    "\n",
    "# Loop through each regularization configuration\n",
    "for config in regularization_configs:\n",
    "    l2_penalty = config['l2_penalty']\n",
    "    dropout_rate = config['dropout_rate']\n",
    "    print(f\"\\nTraining model with L2 penalty={l2_penalty} and Dropout rate={dropout_rate}\")\n",
    "    \n",
    "    nn_model = create_neural_network4(l2_penalty=l2_penalty, dropout_rate=dropout_rate)\n",
    "    history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    # Plot training and validation loss and accuracy\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss with L2={l2_penalty} and Dropout={dropout_rate}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy with L2={l2_penalty} and Dropout={dropout_rate}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        - Summarize your experiments with using a graphical representation such as Figure 6.15 [on this page](https://egallic.fr/Enseignement/ML/ECB/book/deep-learning.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](<Screenshot 2024-11-09 230623.png>) ![alt text](<Screenshot 2024-11-09 230734.png>) ![alt text](<Screenshot 2024-11-09 230846.png>) ![alt text](<Screenshot 2024-11-09 231000.png>) ![alt text](<Screenshot 2024-11-09 231114.png>) ![alt text](<Screenshot 2024-11-09 231222.png>) ![alt text](<Screenshot 2024-11-09 231333.png>) ![alt text](<Screenshot 2024-11-09 231440.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "9. **Model Comparison**:\n",
    "\n",
    "    - Evaluate the baseline model on the test set, using the optimal parameter set identified through grid search. Additionally, apply your best-performing neural network configuration to the test set.\n",
    "\n",
    "    - Quantify the performance of the baseline model (best hyperparameter configuration) and your neural network (best configuration) using precision, recall, and F1-score as metrics. How do these two models compare to the dummy model?\n",
    "\n",
    "    - Provide recommendations on which model(s) to choose for this task and justify your choices based on the analysis results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best Decision Tree model hyperparameters were found in a previous step so we used these for the Model Comparison.\n",
    "\n",
    "The Neural Network seemed to perform best when:<br>\n",
    "There were 2 hidden layers<br>\n",
    "There were 8 nodes per layer<br>\n",
    "An L2 penalty of 0.01 and with a dropout rate of 0.25\n",
    "\n",
    "We used 30 epochs as some of these showed slight dropoffs after that and mainly had their peaks in accuracy within the 20-30 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code cell\n",
    "# Step 9\n",
    "\n",
    "# Decision Tree model with specified hyperparameters\n",
    "decision_tree_model = DecisionTreeClassifier(criterion='entropy', min_samples_split=5, max_depth=10)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation/test set\n",
    "y_pred_dt = decision_tree_model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "precision_dt = precision_score(y_test, y_pred_dt, average='weighted', zero_division=1)\n",
    "recall_dt = recall_score(y_test, y_pred_dt, average='weighted')\n",
    "f1_dt = f1_score(y_test, y_pred_dt, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(\"Decision Tree Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_dt)\n",
    "print(\"Precision:\", precision_dt)\n",
    "print(\"Recall:\", recall_dt)\n",
    "print(\"F1 Score:\", f1_dt)\n",
    "\n",
    "# Define the neural network structure\n",
    "def create_neural_network5(num_layers, l2_penalty, dropout_rate, nodes_per_layer):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(462,)))\n",
    "    for _ in range(num_layers):\n",
    "        model.add(Dense(nodes_per_layer, activation='relu', kernel_regularizer=l2(l2_penalty)))  # L2 regularization on the hidden layer\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "neural_network_model = create_neural_network5(l2_penalty=0.01, dropout_rate=0.25, nodes_per_layer=8, num_layers=2)\n",
    "\n",
    "# Train the model (adjust epochs and batch size as needed)\n",
    "history = neural_network_model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict and evaluate the model\n",
    "y_pred_nn = neural_network_model.predict(X_test).argmax(axis=1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "precision_nn = precision_score(y_test, y_pred_nn, average=\"weighted\", zero_division=1)\n",
    "recall_nn = recall_score(y_test, y_pred_nn, average='weighted')\n",
    "f1_nn = f1_score(y_test, y_pred_nn, average='weighted')\n",
    "\n",
    "# Print metrics\n",
    "print(\"Neural Network Model Evaluation\")\n",
    "print(\"Accuracy:\", accuracy_nn)\n",
    "print(\"Precision:\", precision_nn)\n",
    "print(\"Recall:\", recall_nn)\n",
    "print(\"F1 Score:\", f1_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Model Evaluation<br>\n",
    "Accuracy: 0.5589343379978472<br>\n",
    "Precision: 0.5615071732072793<br>\n",
    "Recall: 0.5589343379978472<br>\n",
    "F1 Score: 0.555584610114814<br>\n",
    "\n",
    "Neural Network Model Evaluation<br>\n",
    "Accuracy: 0.7070775026910656<br>\n",
    "Precision: 0.707159434552183<br>\n",
    "Recall: 0.7070775026910656<br>\n",
    "F1 Score: 0.7070984732289461"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources<br>\n",
    "https://stackoverflow.com/questions/29576430/shuffle-dataframe-rows<br>\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html <br>\n",
    "https://www.geeksforgeeks.org/implementing-neural-networks-using-tensorflow/<br>\n",
    "https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.GridSearchCV.html<br>\n",
    "\n",
    "AI Code given when asked how to display loss and accuracy for each set of nodes in step 8 part 1 (Used mainly to save time and to keep code clean and consistent for similar parts):<br>\n",
    "<br>\n",
    "\n",
    "for nodes in node_counts:\n",
    "    print(f\"\\nTraining model with {nodes} hidden nodes...\")\n",
    "    nn_model = create_neural_network(hidden_nodes=nodes)\n",
    "    history = nn_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=0)\n",
    "    \n",
    "    # Plot training and validation loss and accuracy\n",
    "    epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "    plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "    plt.title(f'Loss for {nodes} Nodes in Hidden Layer')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, history.history['accuracy'], 'b', label='Training Accuracy')\n",
    "    plt.plot(epochs, history.history['val_accuracy'], 'r', label='Validation Accuracy')\n",
    "    plt.title(f'Accuracy for {nodes} Nodes in Hidden Layer')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
